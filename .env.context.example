# LLM Provider Configuration
# Choose between 'ollama', 'openai', 'groq', 'bedrock', or 'openwebui'
PROVIDER=ollama

# Model Configuration
# For Ollama models, recommended options:
# - deepseek-coder:1.3b  (Lightest and fastest, code-focused)
# - phi:2b               (Microsoft's light model, good code understanding)
# - codellama:7b         (Meta's code-focused model, good balance)
# - deepseek-r1:7b       (Good code understanding, medium size)
# - qwen2.5:0.5b         (Very small but effective model)
# - mistral:7b           (General purpose but good at code understanding)
# - mixtral-8x7b:instruct (Larger but high quality)
# - llama3.1:8b          (Meta's latest model, good performance)
#
# For OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# For Groq: llama3-8b-8192, mixtral-8x7b-32768
# For Bedrock: amazon.titan-text-express-v1, anthropic.claude-3-sonnet-20240229
# For OpenWebUI: Same as Ollama models
LLM_MODEL=codellama:7b

# Provider-specific configurations

# Ollama Configuration (if PROVIDER=ollama)
OLLAMA_API_URL=http://localhost:11434

# OpenAI Configuration (if PROVIDER=openai)
# OPENAI_API_KEY=sk-xxx
# OPENAI_API_URL=https://api.openai.com/v1

# Groq Configuration (if PROVIDER=groq)
# GROQ_API_KEY=gsk_xxx
# GROQ_API_URL=https://api.groq.com/v1

# AWS Bedrock Configuration (if PROVIDER=bedrock)
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=xxx
# AWS_REGION=us-east-1
# BEDROCK_API_URL=https://bedrock-runtime.{region}.amazonaws.com/v1

# OpenWebUI Configuration (if PROVIDER=openwebui)
# OPENWEBUI_API_URL=http://localhost:3000/v1
# OPENWEBUI_API_KEY=sk-xxx

# LLM Parameters
TEMPERATURE=0.7
MAX_TOKENS=30000

# Application settings
UPDATE_INTERVAL=5000
OUTPUT_PATH=project-doc

# Model Selection Notes:
# - Small models (1-4B): Run fast on minimal systems, sufficient for simple projects
# - Medium models (7-8B): Good balance, sufficient for most projects
# - Large models (>8B): Best output quality but slower, for complex projects
# 
# Best performing models for code documentation (from smallest to largest):
# 1. qwen2.5:0.5b - Very lightweight option, good for minimal systems
# 2. deepseek-coder:1.3b - Lightweight option, code-focused
# 3. phi:2b - Microsoft's small model, good memory usage
# 4. codellama:7b - Specially trained for code, good for detailed analysis
# 5. deepseek-r1:7b - Strong general code understanding
# 6. llama3.1:8b - Meta's latest model, high quality
# 7. mixtral-8x7b - Most comprehensive analysis but slowest option